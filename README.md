# âœ‹ Task 4: Hand Gesture Recognition (CNN)

*Prodigy Infotech Internship - Machine Learning Track*

### ðŸ“Œ Project Overview
This project implements a *Convolutional Neural Network (CNN)* to identify and classify hand gestures from images. 

While the original task description mentions the LeapGestRecog dataset, this implementation uses the *Rock-Paper-Scissors* dataset from TensorFlow. This dataset serves as a standard, lightweight alternative that allows for efficient training of deep learning models in cloud environments like Google Colab without the overhead of processing 2GB+ of raw data.

### ðŸ“Š Model Output
The model was trained for 10 epochs. The graphs below show the training progress, achieving high accuracy on both training and validation sets.

<img width="1626" height="257" alt="image" src="https://github.com/user-attachments/assets/a4740231-b9c8-4f84-a0cd-696065b7ca22" />
<img width="1118" height="728" alt="image" src="https://github.com/user-attachments/assets/e41dde33-705b-4551-b536-ad0bf298f80e" />

### ðŸ›  Technologies Used
* *Python*
* *TensorFlow & Keras* (Deep Learning Framework)
* *OpenCV / SciKit-Image* (Image Processing)
* *Matplotlib* (Visualization)

### ðŸ“‚ Dataset
* *Source:* [Rock-Paper-Scissors Dataset (TensorFlow)](https://www.tensorflow.org/datasets/catalog/rock_paper_scissors)
* *Structure:* The dataset contains synthetic images of hands in different poses, rendered with various lighting conditions to simulate real-world diversity.

### ðŸš€ How to Run
1.  Open PRODIGY_ML_04.ipynb in Google Colab.
2.  Run the notebook cells. The script will automatically download the dataset and train the CNN model.
3.  Scroll to the bottom to see a test prediction on a sample image.
